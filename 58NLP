1、关键词提取的方法？
    最基础的tfidf
1.1、别的呢？
    不熟
2、Attention?
    QKV，multi-head，add&norm
3、深度学习的正则化，它是怎么起作用的？
    dropout算一种，起集成作用
3.1、别的呢？有几种正则化？L1 L2的数学原理？
    画了一下L1方形和L2圆形那个图，把范数加到损失函数里一起最小化，L1交点在坐标轴诱导稀疏，L2让参数更小
4、自己动手写过深度学习吗？
    没
5、你了解CRF吗？
    无向概率图，团势函数，归一化，推导太复杂看了几次没看透
6、HMM呢？
    做过音字转换
6.1、转移概率和发射概率是怎么算的？
    在语料库上统计bigram
7、文本摘要了解吗？
    no
8、多轮对话？
    bert能用
9、word2vec有没有用过？
    onehot算不了相似度，转成低维稠密向量就能算了，你问我用没用过那我只好回答没用过
9.1、怎么训练？
    cbow，skip-gram，bert，目标是中心词预测上下文或者上下文预测中心词，训练完成后得到embedding
10、对推荐系统有没有了解？
    content based，协同过滤
11、SVD的了解？
    高维稀疏矩阵降维，便于计算
11.1、在数学是上怎么表示的？
    分解为三个矩阵的乘积，别的忘了
12、文本相似度有什么度量方法？
    consine相似度最常用
12.1、cosine相似度的公式？
    分子是点积，分母是模长，归一化之后直接算点积
13、100万篇文章算两两相似度，怎么比较快？
    像刚才说的归一化
13.1、归一化之后有什么方案？
    （可能溜号了，听成一个doc跟100万个算相似）不全算，只挑邻近的少量doc跟目标doc算相似
13.2、怎么选邻近的？
    knn，局部敏感哈希
13.3、局部敏感哈希有很多种，是哪种？
    种类不知道，在算的过程中能把相似的doc分到同一个桶里
13.4、怎么算的？
    ……
13.5、其实是想问归一化之后两两算相似度怎么更快
    （当作矩阵乘法，分布式并行加速，主要是没听到“两两都要算相似”，要不然这个我知道。。。）
14、有没有用过spark？
    用过hadoop
15、算法题：2000万个float，选topk
    最小堆
15.1、复杂度？
    O(Nlogk)
15.2、有O(n)的吗？
    ？？？
15.3、快排的复杂度？
    O(NlogN)
15.4、那topk能用partition吗？
    哦。。。不要求有序可以用partition达到O(N)
16、有啥问我的？
    做啥业务，用啥技术
